{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cbb53b6-a9fc-494c-9545-5bc992e32e13",
   "metadata": {},
   "source": [
    "# Prompts Templating for Ollama - LangChain\n",
    "Until 2021, to use an AI model for a specific use-case we would need to fine-tune the model weights themselves. That would require huge amounts of training data and significant compute to fine-tune any reasonably performing model.\n",
    "\n",
    "Instruction fine-tuned Large Language Models (LLMs) changed this fundamental rule of applying AI models to new use-cases. Rather than needing to either train a model from scratch or fine-tune an existing model, these new LLMs could adapt incredibly well to a new problem or use-case with nothing more than a prompt change.\n",
    "\n",
    "Prompts allow us to completely change the functionality of an AI pipeline. Through natural language we simply tell our LLM what it needs to do, and with the right AI pipeline and prompting, it often works.\n",
    "\n",
    "LangChain naturally has many functionalities geared towards helping us build our prompts. We can build very dynamic prompting pipelines that modifying the structure and content of what we feed into our LLM based on essentially any parameter we would like. In this example, we'll explore the essentials to prompting in LangChain and apply this in a demo Retrieval Augmented Generation (RAG) pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba27ca0a-b672-454f-a836-b90841649c50",
   "metadata": {},
   "source": [
    "# Basic Prompting\n",
    "We'll start by looking at the various parts of our prompt. For RAG use-cases we'll typically have three core components however this is very use-cases dependant and can vary significantly. Nonetheless, for RAG we will typically see:\n",
    "\n",
    "__Rules for our LLM:__ this part of the prompt sets up the behavior of our LLM, how it should approach responding to user queries, and simply providing as much information as possible about what we're wanting to do as possible. We typically place this within the system prompt of an chat LLM.\n",
    "\n",
    "__Context:__ this part is RAG-specific. The context refers to some external information that we may have retrieved from a web search, database query, or often a vector database. This external information is the Retrieval Augmentation part of RAG. For chat LLMs we'll typically place this inside the chat messages between the assistant and user.\n",
    "\n",
    "__Question:__ this is the input from our user. In the vast majority of cases the question/query/user input will always be provided to the LLM (and typically through a user message). However, the format and location of this being provided often changes.\n",
    "\n",
    "__Answer:__ this is the answer from our assistant, again this is very typical and we'd expect this with every use-case.\n",
    "\n",
    "The below is an example of how a RAG prompt may look:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7504cd97-aa99-4894-bcd6-6361d19c1440",
   "metadata": {},
   "source": [
    "Answer the question based on the context below,                 }\n",
    "if you cannot answer the question using the                     }--->  (Rules) For Our Prompt\n",
    "provided information answer with \"I don't know\"                 }\n",
    "\n",
    "Context: Aurelio AI is an AI development studio                 }\n",
    "focused on the fields of Natural Language Processing (NLP)      }\n",
    "and information retrieval using modern tooling                  }--->   Context AI has\n",
    "such as Large Language Models (LLMs),                           }\n",
    "vector databases, and LangChain.                                }\n",
    "\n",
    "Question: Does Aurelio AI do anything related to LangChain?     }--->   User Question\n",
    "\n",
    "Answer:                                                         }--->   AI Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fede66-d16e-4a3d-bcc7-9053d17e26c3",
   "metadata": {},
   "source": [
    "Here we can see how the AI will appoach our question, as you can see we have a formulated response, if the context has the answer, then use the context to answer the question, if not, say I don't know, then we also have context and question which are being passed into this similarly to paramaters in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a12bb2d1-c1e4-458f-81f1-61e66605a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Answer the user's query based on the context below.\n",
    "If you cannot answer the question using the\n",
    "provided information answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00de265d-8a80-4f34-bb5f-633b93110338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# passing the template to the LangChain model\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", prompt),\n",
    "    (\"user\", \"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293ce3bf-04aa-43d8-bdd8-fac9b81c2fe4",
   "metadata": {},
   "source": [
    "When we call the template it will expect us to provide two variables, the context and the query. Both of these variables are pulled from the strings we wrote, as LangChain interprets curly-bracket syntax (ie {context} and {query}) as indicating a dynamic variable that we expect to be inserted at query time. We can see that these variables have been picked up by our template object by viewing it's input_variables attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7269279-0899-4493-895e-87b42bfe0bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['context', 'query']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61abf12c-ed54-4ace-bcbd-c9c5c96cbe11",
   "metadata": {},
   "source": [
    "We can also view the structure of the messages (currently prompt templates) that the ChatPromptTemplate will construct by viewing the messages attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92710fb2-242b-488d-897d-0923a1afafef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nContext: {context}\\n'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c62e595-7259-4e44-b433-5e6082584b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(prompt),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ad5cbc9-73d4-48d0-9729-afd085aae0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the user\\'s query based on the context below.\\nIf you cannot answer the question using the\\nprovided information answer with \"I don\\'t know\".\\n\\nContext: {context}\\n'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c3683f4-e491-429c-b629-9d12e73c5524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# For normal accurate responses\n",
    "llm = ChatOllama(temperature=0.0, model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf3d3075-ed53-44c5-ada1-829d667631da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = (\n",
    "    {\n",
    "        \"query\": lambda x: x[\"query\"],\n",
    "        \"context\": lambda x: x[\"context\"]\n",
    "    }\n",
    "    | prompt_template\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ee74926-b226-474d-8790-8e28a2846062",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"Aurelio AI is an AI company developing tooling for AI\n",
    "engineers. Their focus is on language AI with the team having strong\n",
    "expertise in building AI agents and a strong background in\n",
    "information retrieval.\n",
    "\n",
    "The company is behind several open source frameworks, most notably\n",
    "Semantic Router and Semantic Chunkers. They also have an AI\n",
    "Platform providing engineers with tooling to help them build with\n",
    "AI. Finally, the team also provides development services to other\n",
    "organizations to help them bring their AI tech to market.\n",
    "\n",
    "Aurelio AI became LangChain Experts in September 2024 after a long\n",
    "track record of delivering AI solutions built with the LangChain\n",
    "ecosystem.\"\"\"\n",
    "\n",
    "query = \"what does Aurelio AI do?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f209913-daa0-4f5c-af9d-0ceb6cdac30c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Aurelio AI is an AI company that develops tooling for AI engineers, primarily focusing on language AI. They provide various tools and services to help build and deploy AI models, including:\\n\\n1. Open-source frameworks (e.g., Semantic Router and Semantic Chunkers)\\n2. An AI Platform for building with AI\\n3. Development services to help other organizations bring their AI tech to market\\n\\nThey also gained the title of LangChain Experts in September 2024, indicating a strong expertise in the LangChain ecosystem.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-05-14T05:55:55.1818144Z', 'done': True, 'done_reason': 'stop', 'total_duration': 12703891800, 'load_duration': 31710000, 'prompt_eval_count': 201, 'prompt_eval_duration': 627385100, 'eval_count': 105, 'eval_duration': 12041384100, 'model_name': 'llama3.2'}, id='run--f06508c6-a20a-471a-b93d-dbc42f595cd6-0', usage_metadata={'input_tokens': 201, 'output_tokens': 105, 'total_tokens': 306})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.invoke({\"query\": query, \"context\": context})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52d443f-b600-4d03-b706-31997e85582d",
   "metadata": {},
   "source": [
    "# Few Shot Prompting\n",
    "Many State-of-the-Art (SotA) LLMs are incredible at instruction following. Meaning that it requires much less effort to get the intended output or behavior from these models than is the case for older LLMs and smaller LLMs.\n",
    "\n",
    "Before creating an example let's first see how to use LangChain's few shot prompting objects. We will provide multiple examples and we'll feed them in as sequential human and ai messages so we setup the template like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ec7b21cb-878a-47bd-bfab-d58aa86eb7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74697c07-b5ea-489b-bf4a-df061ff594ef",
   "metadata": {},
   "source": [
    "Then we define a list of examples with dictionaries containing the correct input and output keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d5eed0f-f010-44be-9696-3eaeb6bec1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"input\": \"Here is query #1\", \"output\": \"Here is the answer #1\"},\n",
    "    {\"input\": \"Here is query #2\", \"output\": \"Here is the answer #2\"},\n",
    "    {\"input\": \"Here is query #3\", \"output\": \"Here is the answer #3\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "746d69d1-95df-4365-83ac-a3c905112e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Here is query #1\n",
      "AI: Here is the answer #1\n",
      "Human: Here is query #2\n",
      "AI: Here is the answer #2\n",
      "Human: Here is query #3\n",
      "AI: Here is the answer #3\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "# here is the formatted prompt\n",
    "print(few_shot_prompt.format())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5e9a06-3d43-488f-b2c9-b49cf8ffff52",
   "metadata": {},
   "source": [
    "# Few-Shot Example\n",
    "Using a tiny LLM limits it's ability, so when asking for specific behaviors or structured outputs it can struggle. For example, we'll ask the LLM to summarize the key points about Aurelio AI using markdown and bullet points. Let's see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e1e857e7-c063-4d42-b041-76459afc86f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Overview of Aurelio AI**\n",
      "==========================\n",
      "\n",
      "Aurelio AI is an AI company that develops tooling for AI engineers, specializing in language AI. They provide a range of services and products to help build and deploy AI solutions.\n",
      "\n",
      "### Key Activities:\n",
      "\n",
      "*   Developing open-source frameworks (e.g., Semantic Router and Semantic Chunkers)\n",
      "*   Creating an AI Platform for building with AI\n",
      "*   Offering development services to other organizations\n",
      "\n",
      "### Expertise:\n",
      "\n",
      "*   Strong expertise in building AI agents\n",
      "*   Background in information retrieval\n"
     ]
    }
   ],
   "source": [
    "new_system_prompt = \"\"\"\n",
    "Answer the user's query based on the context below.\n",
    "If you cannot answer the question using the\n",
    "provided information answer with \"I don't know\".\n",
    "\n",
    "Always answer in markdown format. When doing so please\n",
    "provide headers, short summaries, follow with bullet\n",
    "points, then conclude.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template.messages[0].prompt.template = new_system_prompt\n",
    "\n",
    "out = pipeline.invoke({\"query\": query, \"context\": context}).content\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52adb1d7-80c8-4381-9ef0-883ec8376295",
   "metadata": {},
   "source": [
    "We can display our markdown nicely with IPython like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c04a6ea3-9d1b-4f68-8399-ee2b431087da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Overview of Aurelio AI**\n",
       "==========================\n",
       "\n",
       "Aurelio AI is an AI company that develops tooling for AI engineers, specializing in language AI. They provide a range of services and products to help build and deploy AI solutions.\n",
       "\n",
       "### Key Activities:\n",
       "\n",
       "*   Developing open-source frameworks (e.g., Semantic Router and Semantic Chunkers)\n",
       "*   Creating an AI Platform for building with AI\n",
       "*   Offering development services to other organizations\n",
       "\n",
       "### Expertise:\n",
       "\n",
       "*   Strong expertise in building AI agents\n",
       "*   Background in information retrieval"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(out))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c09a710f-4e5e-4cb8-b963-54d1f620dbe8",
   "metadata": {},
   "source": [
    "This is not bad, but also not quite the format we wanted. We could try improving our initial prompting instructions, but when this doesn't work we can move on to our few-shot prompting. We want to build something like this:\n",
    "\n",
    "Answer the user's query based on the context below,                 }\n",
    "if you cannot answer the question using the                         }\n",
    "provided information answer with \"I don't know\"                     }\n",
    "                                                                    }--->  (Rules)\n",
    "Always answer in markdown format. When doing so please              }\n",
    "provide headers, short summaries, follow with bullet                }\n",
    "points, then conclude. Here are some examples:                      }\n",
    "\n",
    "\n",
    "User: Can you explain gravity?                                      }\n",
    "AI: ## Gravity                                                      }\n",
    "                                                                    }\n",
    "Gravity is one of the fundamental forces in the universe.           }\n",
    "                                                                    }\n",
    "### Discovery                                                       }--->  (Example 1)\n",
    "                                                                    }\n",
    "* Gravity was first discovered by...                                }\n",
    "                                                                    }\n",
    "**To conclude**, Gravity is a fascinating topic and has been...     }\n",
    "                                                                    }\n",
    "\n",
    "User: What is the capital of France?                                }\n",
    "AI: ## France                                                       }\n",
    "                                                                    }\n",
    "The capital of France is Paris.                                     }\n",
    "                                                                    }--->  (Example 2)\n",
    "### Origins                                                         }\n",
    "                                                                    }\n",
    "* The name Paris comes from the...                                  }\n",
    "                                                                    }\n",
    "**To conclude**, Paris is highly regarded as one of the...          }\n",
    "\n",
    "Context: {context}                                                  }--->  (Context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b03b5bc2-5457-4769-8920-b4515cfa7d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Can you explain gravity?\",\n",
    "        \"output\": (\n",
    "            \"## Gravity\\n\\n\"\n",
    "            \"Gravity is one of the fundamental forces in the universe.\\n\\n\"\n",
    "            \"### Discovery\\n\\n\"\n",
    "            \"* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\\n\"\n",
    "            \"* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\\n\\n\"\n",
    "            \"### In General Relativity\\n\\n\"\n",
    "            \"* Gravity is described as the curvature of spacetime.\\n\"\n",
    "            \"* The more massive an object is, the more it curves spacetime.\\n\"\n",
    "            \"* This curvature is what causes objects to fall towards each other.\\n\\n\"\n",
    "            \"### Gravitons\\n\\n\"\n",
    "            \"* Gravitons are hypothetical particles that mediate the force of gravity.\\n\"\n",
    "            \"* They have not yet been detected.\\n\\n\"\n",
    "            \"**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\\n\\n\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is the capital of France?\",\n",
    "        \"output\": (\n",
    "            \"## France\\n\\n\"\n",
    "            \"The capital of France is Paris.\\n\\n\"\n",
    "            \"### Origins\\n\\n\"\n",
    "            \"* The name Paris comes from the Latin word \\\"Parisini\\\" which referred to a Celtic people living in the area.\\n\"\n",
    "            \"* The Romans named the city Lutetia, which means \\\"the place where the river turns\\\".\\n\"\n",
    "            \"* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\\n\\n\"\n",
    "            \"**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world's greatest cultural and economic centres.\\n\\n\"\n",
    "        )\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ac2d2e5-4eb4-4169-8958-b5b62d808b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bafea37d-93aa-4181-b1df-541226b2bb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Human: Can you explain gravity?\n",
       "AI: ## Gravity\n",
       "\n",
       "Gravity is one of the fundamental forces in the universe.\n",
       "\n",
       "### Discovery\n",
       "\n",
       "* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\n",
       "* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\n",
       "\n",
       "### In General Relativity\n",
       "\n",
       "* Gravity is described as the curvature of spacetime.\n",
       "* The more massive an object is, the more it curves spacetime.\n",
       "* This curvature is what causes objects to fall towards each other.\n",
       "\n",
       "### Gravitons\n",
       "\n",
       "* Gravitons are hypothetical particles that mediate the force of gravity.\n",
       "* They have not yet been detected.\n",
       "\n",
       "**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\n",
       "\n",
       "\n",
       "Human: What is the capital of France?\n",
       "AI: ## France\n",
       "\n",
       "The capital of France is Paris.\n",
       "\n",
       "### Origins\n",
       "\n",
       "* The name Paris comes from the Latin word \"Parisini\" which referred to a Celtic people living in the area.\n",
       "* The Romans named the city Lutetia, which means \"the place where the river turns\".\n",
       "* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\n",
       "\n",
       "**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world's greatest cultural and economic centres.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = few_shot_prompt.format()\n",
    "\n",
    "display(Markdown(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f41b78d5-c736-428d-8bf4-77b32b715354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotChatMessagePromptTemplate(examples=[{'input': 'Can you explain gravity?', 'output': '## Gravity\\n\\nGravity is one of the fundamental forces in the universe.\\n\\n### Discovery\\n\\n* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\\n* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\\n\\n### In General Relativity\\n\\n* Gravity is described as the curvature of spacetime.\\n* The more massive an object is, the more it curves spacetime.\\n* This curvature is what causes objects to fall towards each other.\\n\\n### Gravitons\\n\\n* Gravitons are hypothetical particles that mediate the force of gravity.\\n* They have not yet been detected.\\n\\n**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\\n\\n'}, {'input': 'What is the capital of France?', 'output': '## France\\n\\nThe capital of France is Paris.\\n\\n### Origins\\n\\n* The name Paris comes from the Latin word \"Parisini\" which referred to a Celtic people living in the area.\\n* The Romans named the city Lutetia, which means \"the place where the river turns\".\\n* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\\n\\n**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world\\'s greatest cultural and economic centres.\\n\\n'}], input_variables=[], input_types={}, partial_variables={}, example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "87bb4352-aa76-4a5a-b268-08b34a2ffe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", new_system_prompt),\n",
    "    few_shot_prompt,\n",
    "    (\"user\", \"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7d306757-ddc7-4cab-8ba2-aa986bb38dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Aurelio AI\n",
       "\n",
       "Aurelio AI is an AI company that develops tooling for AI engineers, with a focus on language AI.\n",
       "\n",
       "### Key Products and Services\n",
       "\n",
       "* **Semantic Router**: An open-source framework for building conversational interfaces.\n",
       "* **Semantic Chunkers**: Another open-source framework for chunking text into meaningful units.\n",
       "* **AI Platform**: A platform providing tooling to help engineers build with AI.\n",
       "* **Development Services**: Aurelio AI provides development services to other organizations to help them bring their AI tech to market.\n",
       "\n",
       "### Expertise\n",
       "\n",
       "* **Language AI**: The team has strong expertise in building AI agents and a background in information retrieval.\n",
       "\n",
       "**To conclude**, Aurelio AI is a company that specializes in developing tooling for language AI and providing services to help organizations build with AI."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = prompt_template | llm\n",
    "out = pipeline.invoke({\"query\": query, \"context\": context}).content\n",
    "display(Markdown(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecfbd9b-6bff-45f8-b81c-553851dad520",
   "metadata": {},
   "source": [
    "We can see that by adding a few examples to our prompt, ie few-shot prompting, we can get much more control over the exact structure of our LLM response. As the size of our LLMs increases, the ability of them to follow instructions becomes much greater and they tend to require less explicit prompting as we have shown here. However, even for SotA models like gpt-4o few-shot prompting is still a valid technique that can be used if the LLM is struggling to follow our intended instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cf595e-2d49-45da-9e44-99defd1dcdd3",
   "metadata": {},
   "source": [
    "# Chain of Thought Prompting\n",
    "We'll take a look at one more commonly used prompting technique called chain of thought (CoT). CoT is a technique that encourages the LLM to think through the problem step by step before providing an answer. The idea being that by breaking down the problem into smaller steps, the LLM is more likely to arrive at the correct answer and we are less likely to see hallucinations.\n",
    "\n",
    "To implement CoT we don't need any specific LangChain objects, instead we are simply modifying how we instruct our LLM within the system prompt. We will ask the LLM to list the problems that need to be solved, to solve each problem individually, and then to arrive at the final answer.\n",
    "\n",
    "Let's first test our LLM without CoT prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "99dafe2d-16cf-46cf-9b5d-9b9279e9d91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cot_system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\n",
    "You MUST answer the question directly without any other\n",
    "text or explanation.\n",
    "\"\"\"\n",
    "\n",
    "no_cot_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", no_cot_system_prompt),\n",
    "    (\"user\", \"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1ba2d67a-1e3d-446f-ad3f-8319e9138d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,930\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"How many keystrokes are needed to type the numbers from 1 to 500?\"\n",
    ")\n",
    "\n",
    "no_cot_pipeline = no_cot_prompt_template | llm\n",
    "no_cot_result = no_cot_pipeline.invoke({\"query\": query}).content\n",
    "print(no_cot_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d94fdb-421f-4af8-bd62-46004edba7d0",
   "metadata": {},
   "source": [
    "The actual answer is 1392, but the LLM without CoT just hallucinates and gives us a guess. Now, we can add explicit CoT prompting to our system prompt to see if we can get a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d99df18e-ce3b-4b69-adc9-0512867e4617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chain-of-thought prompt template\n",
    "cot_system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\n",
    "To answer the question, you must:\n",
    "\n",
    "- List systematically and in precise detail all\n",
    "  subproblems that need to be solved to answer the\n",
    "  question.\n",
    "- Solve each sub problem INDIVIDUALLY and in sequence.\n",
    "- Finally, use everything you have worked through to\n",
    "  provide the final answer.\n",
    "\"\"\"\n",
    "cot_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", cot_system_prompt),\n",
    "    (\"user\", \"{query}\"),\n",
    "])\n",
    "\n",
    "cot_pipeline = cot_prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7bd3f3e-f0ad-46e4-8798-c0bd20c4b42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To calculate the number of keystrokes needed to type the numbers from 1 to 500, we need to break down the problem into smaller subproblems. Here's a step-by-step approach:\n",
       "\n",
       "**Subproblem 1: Counting single-digit numbers (1-9)**\n",
       "\n",
       "* Each digit requires 1 keystroke.\n",
       "* There are 9 single-digit numbers, so the total number of keystrokes for these numbers is 9.\n",
       "\n",
       "**Subproblem 2: Counting two-digit numbers (10-99)**\n",
       "\n",
       "* Each digit requires 1 keystroke.\n",
       "* For each two-digit number, we need to type:\n",
       "\t+ The tens digit (1 keystroke)\n",
       "\t+ The units digit (1 keystroke)\n",
       "* There are 90 two-digit numbers (from 10 to 99), so the total number of keystrokes for these numbers is 2 x 90 = 180.\n",
       "\n",
       "**Subproblem 3: Counting three-digit numbers (100-500)**\n",
       "\n",
       "* Each digit requires 1 keystroke.\n",
       "* For each three-digit number, we need to type:\n",
       "\t+ The hundreds digit (1 keystroke)\n",
       "\t+ The tens digit (1 keystroke)\n",
       "\t+ The units digit (1 keystroke)\n",
       "* There are 401 three-digit numbers (from 100 to 500), so the total number of keystrokes for these numbers is 3 x 401 = 1203.\n",
       "\n",
       "**Subproblem 4: Counting the number '0'**\n",
       "\n",
       "* We need to type the number '0' once, which requires 1 keystroke.\n",
       "\n",
       "Now, let's add up the results from each subproblem:\n",
       "\n",
       "9 (single-digit numbers) + 180 (two-digit numbers) + 1203 (three-digit numbers) + 1 (number '0') = 1393\n",
       "\n",
       "Therefore, approximately **1393 keystrokes** are needed to type the numbers from 1 to 500."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cot_result = cot_pipeline.invoke({\"query\": query}).content\n",
    "display(Markdown(cot_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d8d91cae-2a18-4f7f-8c5d-acf49532cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Be a helpful assistant and answer the user's question.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"{query}\"),\n",
    "])\n",
    "\n",
    "pipeline = prompt_template | llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d9df4ac0-3b84-420d-90a2-e93bc3dac374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To calculate the number of keystrokes needed to type the numbers from 1 to 500, we need to consider two things:\n",
       "\n",
       "1. The number of digits in each number\n",
       "2. The number of keystrokes required for each digit (assuming a standard QWERTY keyboard layout)\n",
       "\n",
       "Let's break it down:\n",
       "\n",
       "* Single-digit numbers (1-9): 9 numbers x 1 keystroke = 9 keystrokes\n",
       "* Two-digit numbers (10-99): 90 numbers x 2 keystrokes = 180 keystrokes\n",
       "* Three-digit numbers (100-500): 401 numbers x 3 keystrokes = 1203 keystrokes\n",
       "\n",
       "Now, let's add up the total number of keystrokes:\n",
       "\n",
       "9 + 180 + 1203 = 1392 keystrokes\n",
       "\n",
       "So, approximately 1392 keystrokes are needed to type the numbers from 1 to 500."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = pipeline.invoke({\"query\": query}).content\n",
    "display(Markdown(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
