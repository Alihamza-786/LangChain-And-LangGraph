{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "040e11b9-88de-4e85-b169-ae472346ad17",
   "metadata": {},
   "source": [
    "# Traditional Chains vs LCEL\n",
    "In this section we're going to dive into a basic example using the traditional method for building chains before jumping into LCEL. We will build a pipeline where the user must input a specific topic, and then the LLM will look and return a report on the specified topic. Generating a research report for the user.\n",
    "\n",
    "# Traditional LLMChain\n",
    "The LLMChain is the simplest chain originally introduced in LangChain. This chain takes a prompt, feeds it into an LLM, and optionally adds an output parsing step before returning the result.\n",
    "\n",
    "Let's see how we construct this using the traditional method, for this we need:\n",
    "\n",
    "- prompt — a PromptTemplate that will be used to generate the prompt for the LLM.\n",
    "- llm — the LLM we will be using to generate the output.\n",
    "- output_parser — an optional output parser that will be used to parse the structured output of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d4a668e-25b8-4edd-adfa-6203261c4575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = \"Give me a small report on {topic}\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4429fcce-9b9f-4a39-aa42-528409d5e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(\n",
    "    model = \"llama3.2\",\n",
    "    temperatur=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44178b32-5264-45d2-84e1-9607432e69c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"It's nice to meet you. Is there something I can help you with, or would you like to chat for a bit?\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-05-14T12:45:29.8789716Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1345331300, 'load_duration': 25900300, 'prompt_eval_count': 27, 'prompt_eval_duration': 221263600, 'eval_count': 27, 'eval_duration': 1097561300, 'model_name': 'llama3.2'}, id='run--84e5f4f8-b3db-4bc5-b70c-59f3bc98458c-0', usage_metadata={'input_tokens': 27, 'output_tokens': 27, 'total_tokens': 54})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_out = llm.invoke(\"Hello there\")\n",
    "llm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4a8807-2a35-40c4-adc9-f60e8dec8a9f",
   "metadata": {},
   "source": [
    "Then we define our output parser, this will be used to parse the output of the LLM. In this case, we will use the StrOutputParser which will parse the AIMessage output from our LLM into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21e1f2f0-72af-4f6e-b2a9-cc3d0c9b7091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1080f75-8853-420f-b73f-06112bbd6f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's nice to meet you. Is there something I can help you with, or would you like to chat for a bit?\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = output_parser.invoke(llm_out)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef506d2e-96b4-4db2-bd56-55d5c406f56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theha\\AppData\\Local\\Temp\\ipykernel_22056\\2960353250.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23588cb4-f399-459f-9b0a-e1a1587ddf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'retrieval augmented generation',\n",
       " 'text': '**Retrieval-Augmented Generation (RAG)**\\n\\nRetrieval-Augmented Generation is an emerging field in natural language processing (NLP) and artificial intelligence that aims to improve the efficiency and effectiveness of text generation tasks. The term \"retrieval\" refers to the process of retrieving relevant information from a large dataset or knowledge base.\\n\\n**Key Concepts:**\\n\\n1. **Retrieval**: In RAG, retrieval is done using a pre-trained language model (e.g., BERT) that takes in a query and returns a list of relevant documents or tokens.\\n2. **Augmentation**: The retrieved tokens are then used to augment the generated text, which can include adding new words, phrases, or sentences.\\n3. **Generation**: The final step involves generating new text based on the augmented input.\\n\\n**Benefits:**\\n\\n1. **Improved efficiency**: RAG can reduce the number of parameters required for generation, making it more efficient and computationally faster.\\n2. **Better contextual understanding**: By retrieving relevant information from a dataset, RAG can provide a better understanding of the context in which text is being generated.\\n3. **Increased creativity**: The use of retrieved tokens can add new ideas and perspectives to the generated text.\\n\\n**Applications:**\\n\\n1. **Chatbots and conversational AI**: RAG can be used to improve chatbots\\' ability to respond accurately and creatively to user queries.\\n2. **Content generation**: RAG can be applied to generate high-quality content, such as articles or social media posts, by leveraging existing knowledge bases.\\n3. **Question answering**: RAG can be used to improve question-answering systems by retrieving relevant information from a dataset.\\n\\n**Challenges and Limitations:**\\n\\n1. **Scalability**: RAG requires large datasets to train the pre-trained language model and retrieve relevant information.\\n2. **Contextual understanding**: While RAG improves contextual understanding, it may not always capture nuances or subtleties in human communication.\\n3. **Overreliance on data**: RAG relies heavily on the quality of the dataset used for retrieval and augmentation.\\n\\n**Conclusion:**\\n\\nRetrieval-Augmented Generation is an exciting area of research that has the potential to transform natural language processing and artificial intelligence. By leveraging existing knowledge bases, RAG can improve efficiency, contextual understanding, and creativity in text generation tasks. However, there are still challenges to overcome before RAG can be widely applied in real-world applications.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke(\"retrieval augmented generation\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdf3c33f-de0b-4080-bf8d-396311fa01ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Retrieval-Augmented Generation (RAG)**\n",
       "\n",
       "Retrieval-Augmented Generation is an emerging field in natural language processing (NLP) and artificial intelligence that aims to improve the efficiency and effectiveness of text generation tasks. The term \"retrieval\" refers to the process of retrieving relevant information from a large dataset or knowledge base.\n",
       "\n",
       "**Key Concepts:**\n",
       "\n",
       "1. **Retrieval**: In RAG, retrieval is done using a pre-trained language model (e.g., BERT) that takes in a query and returns a list of relevant documents or tokens.\n",
       "2. **Augmentation**: The retrieved tokens are then used to augment the generated text, which can include adding new words, phrases, or sentences.\n",
       "3. **Generation**: The final step involves generating new text based on the augmented input.\n",
       "\n",
       "**Benefits:**\n",
       "\n",
       "1. **Improved efficiency**: RAG can reduce the number of parameters required for generation, making it more efficient and computationally faster.\n",
       "2. **Better contextual understanding**: By retrieving relevant information from a dataset, RAG can provide a better understanding of the context in which text is being generated.\n",
       "3. **Increased creativity**: The use of retrieved tokens can add new ideas and perspectives to the generated text.\n",
       "\n",
       "**Applications:**\n",
       "\n",
       "1. **Chatbots and conversational AI**: RAG can be used to improve chatbots' ability to respond accurately and creatively to user queries.\n",
       "2. **Content generation**: RAG can be applied to generate high-quality content, such as articles or social media posts, by leveraging existing knowledge bases.\n",
       "3. **Question answering**: RAG can be used to improve question-answering systems by retrieving relevant information from a dataset.\n",
       "\n",
       "**Challenges and Limitations:**\n",
       "\n",
       "1. **Scalability**: RAG requires large datasets to train the pre-trained language model and retrieve relevant information.\n",
       "2. **Contextual understanding**: While RAG improves contextual understanding, it may not always capture nuances or subtleties in human communication.\n",
       "3. **Overreliance on data**: RAG relies heavily on the quality of the dataset used for retrieval and augmentation.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "Retrieval-Augmented Generation is an exciting area of research that has the potential to transform natural language processing and artificial intelligence. By leveraging existing knowledge bases, RAG can improve efficiency, contextual understanding, and creativity in text generation tasks. However, there are still challenges to overcome before RAG can be widely applied in real-world applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(result[\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e9ea3-042f-45e6-a534-59eb2f372a3b",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL)\n",
    "LangChain Expression Language (LCEL) is the recommended approach to building chains in LangChain. Having superceeded the traditional methods with LLMChain, etc. LCEL gives us a more flexible system for building chains. The pipe operator | is used by LCEL to chain together components. Let's see how we'd construct an LLMChain using LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3d04a29-6c0c-43cd-9e30-6d5d99e54e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcel_chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5874041b-2085-4b85-8426-5e210723316a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Retrieval Augmented Generation (RAG)**\\n\\nRetrieval Augmented Generation (RAG) is a type of deep learning model that combines the strengths of two popular architectures: Retrieval-based and Generative models. RAG aims to generate text by retrieving relevant information from a large database and then using this retrieved information as input to generate new, coherent text.\\n\\n**Key Components**\\n\\n1. **Retrieval Module**: This module uses a neural network to retrieve relevant documents from a large database based on the input query.\\n2. **Generation Module**: This module generates new text by incorporating the retrieved information into a generator model.\\n3. **Hybrid Model**: The RAG model is a hybrid of the retrieval and generation modules, which work together in a cycle-like fashion.\\n\\n**Advantages**\\n\\n1. **Improved Accuracy**: RAG models can outperform traditional generative models on tasks such as text classification, sentiment analysis, and machine translation.\\n2. **Efficient Use of Data**: By retrieving relevant documents from a database, RAG models can reduce the amount of data required for training compared to generative models.\\n3. **Flexibility**: RAG models can be fine-tuned for specific tasks and domains, allowing them to adapt to new contexts.\\n\\n**Applications**\\n\\n1. **Text Generation**: RAG models have been successfully applied to various text generation tasks, including generating news articles, social media posts, and even entire books.\\n2. **Question Answering**: RAG models can be used to answer questions by retrieving relevant documents from a database.\\n3. **Sentiment Analysis**: RAG models can be used to analyze the sentiment of text data by retrieving relevant documents that contain negative or positive sentiments.\\n\\n**Challenges**\\n\\n1. **Computational Cost**: Training and running RAG models requires significant computational resources due to the large amounts of data involved in retrieval.\\n2. **Data Quality**: The quality of the retrieved documents can significantly impact the performance of the RAG model, requiring high-quality training data.\\n3. **Explainability**: Understanding how RAG models make predictions can be challenging due to the complex interplay between retrieval and generation.\\n\\n**Conclusion**\\n\\nRetrieval Augmented Generation (RAG) is a promising approach for improving the accuracy and efficiency of deep learning models. By leveraging the strengths of both retrieval-based and generative models, RAG has shown great promise in various applications such as text generation, question answering, and sentiment analysis. However, significant challenges remain to be addressed, including computational cost, data quality, and explainability.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = lcel_chain.invoke(\"retrieval augmented generation\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f44d041-a03e-47b4-9ea2-ec24b1e3d5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Retrieval Augmented Generation (RAG)**\n",
       "\n",
       "Retrieval Augmented Generation (RAG) is a type of deep learning model that combines the strengths of two popular architectures: Retrieval-based and Generative models. RAG aims to generate text by retrieving relevant information from a large database and then using this retrieved information as input to generate new, coherent text.\n",
       "\n",
       "**Key Components**\n",
       "\n",
       "1. **Retrieval Module**: This module uses a neural network to retrieve relevant documents from a large database based on the input query.\n",
       "2. **Generation Module**: This module generates new text by incorporating the retrieved information into a generator model.\n",
       "3. **Hybrid Model**: The RAG model is a hybrid of the retrieval and generation modules, which work together in a cycle-like fashion.\n",
       "\n",
       "**Advantages**\n",
       "\n",
       "1. **Improved Accuracy**: RAG models can outperform traditional generative models on tasks such as text classification, sentiment analysis, and machine translation.\n",
       "2. **Efficient Use of Data**: By retrieving relevant documents from a database, RAG models can reduce the amount of data required for training compared to generative models.\n",
       "3. **Flexibility**: RAG models can be fine-tuned for specific tasks and domains, allowing them to adapt to new contexts.\n",
       "\n",
       "**Applications**\n",
       "\n",
       "1. **Text Generation**: RAG models have been successfully applied to various text generation tasks, including generating news articles, social media posts, and even entire books.\n",
       "2. **Question Answering**: RAG models can be used to answer questions by retrieving relevant documents from a database.\n",
       "3. **Sentiment Analysis**: RAG models can be used to analyze the sentiment of text data by retrieving relevant documents that contain negative or positive sentiments.\n",
       "\n",
       "**Challenges**\n",
       "\n",
       "1. **Computational Cost**: Training and running RAG models requires significant computational resources due to the large amounts of data involved in retrieval.\n",
       "2. **Data Quality**: The quality of the retrieved documents can significantly impact the performance of the RAG model, requiring high-quality training data.\n",
       "3. **Explainability**: Understanding how RAG models make predictions can be challenging due to the complex interplay between retrieval and generation.\n",
       "\n",
       "**Conclusion**\n",
       "\n",
       "Retrieval Augmented Generation (RAG) is a promising approach for improving the accuracy and efficiency of deep learning models. By leveraging the strengths of both retrieval-based and generative models, RAG has shown great promise in various applications such as text generation, question answering, and sentiment analysis. However, significant challenges remain to be addressed, including computational cost, data quality, and explainability."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cf01ce-244d-4ee4-b6b3-2a6482b68819",
   "metadata": {},
   "source": [
    "# How Does the Pipe Operator Work?\n",
    "Before moving onto other LCEL features, let's take a moment to understand what the pipe operator | is doing and how it works.\n",
    "\n",
    "Functionality wise, the pipe tells you that whatever the left side outputs will be fed as input into the right side. In the example of prompt | llm | output_parser, we see that prompt feeds into llm feeds into output_parser.\n",
    "\n",
    "The pipe operator is a way of chaining together components, and is a way of saying that whatever the left side outputs will be fed as input into the right side.\n",
    "\n",
    "Let's make a basic class named Runnable that will transform our a provided function into a runnable class that we will then use with the pipe | operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "829f3936-be52-4012-aebe-c5822e7d34c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runnable:\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "    def __or__(self, other):\n",
    "        def chained_func(*args, **kwargs):\n",
    "            return other.invoke(self.func(*args, **kwargs))\n",
    "        return Runnable(chained_func)\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        return self.func(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a4f9cb31-b52e-4683-8a8d-33f884b172d3",
   "metadata": {},
   "source": [
    "With the Runnable class, we will be able wrap a function into the class, allowing us to then chain together multiple of these runnable functions using the __or__ method.\n",
    "\n",
    "First, let's create a few functions that we'll chain together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4254b55d-83bc-4556-8654-8ec0e325d210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_five(x):\n",
    "    return x+5\n",
    "\n",
    "def sub_five(x):\n",
    "    return x-5\n",
    "\n",
    "def mul_five(x):\n",
    "    return x*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "406f24a7-e1d4-460a-a98f-6b9d1a8ca365",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_five_runnable = Runnable(add_five)\n",
    "sub_five_runnable = Runnable(sub_five)\n",
    "mul_five_runnable = Runnable(mul_five)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "08396240-afd1-49ae-8193-792942b36623",
   "metadata": {},
   "source": [
    "Finally, we can chain these together using the __or__ method from the Runnable class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a238fc22-0d1c-4faf-b8eb-8dbb29f78875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (add_five_runnable).__or__(sub_five_runnable).__or__(mul_five_runnable)\n",
    "\n",
    "chain.invoke(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38e2658-2ba5-4997-842e-132e61e0fb7c",
   "metadata": {},
   "source": [
    "# LCEL RunnableLambda\n",
    "The RunnableLambda class is LangChain's built-in method for constructing a runnable object from a function. That is, it does the same thing as the custom Runnable class we created earlier. Let's try it out with the same functions as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "327ebd59-95d4-41fd-afa9-adb6e376be90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "add_five_runnable = RunnableLambda(add_five)\n",
    "sub_five_runnable = RunnableLambda(sub_five)\n",
    "mul_five_runnable = RunnableLambda(mul_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a27c660-0402-4cf6-a1ef-700db7aa56bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = add_five_runnable | sub_five_runnable | mul_five_runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f3db69f-3f2c-4644-bb89-657398591a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(3)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f94f9e62-351d-4a27-b334-4b370509e829",
   "metadata": {},
   "source": [
    "Now we want to try something a little more testing, so this time we will generate a report, and we will try and edit that report using this functionallity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4032b9d-97ff-428f-a28c-e2300e5a1bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"give me a small report about {topic}\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=prompt_str\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4a10460-60ed-4544-992e-2af261fdbd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d892d62-7675-4e86-9eb5-288c076b2447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Artificial Intelligence (AI) Report**\n",
       "\n",
       "**Introduction:**\n",
       "Artificial intelligence (AI) has been rapidly evolving over the past few decades, transforming the way we live and work. From virtual assistants to self-driving cars, AI is increasingly becoming an integral part of our daily lives.\n",
       "\n",
       "**Key Developments in AI:**\n",
       "\n",
       "1. **Machine Learning:** Machine learning algorithms enable computers to learn from data without being explicitly programmed. This has led to significant advancements in areas such as image recognition, natural language processing, and predictive analytics.\n",
       "2. **Deep Learning:** Deep learning techniques, particularly convolutional neural networks (CNNs), have enabled AI systems to excel in tasks like object detection, facial recognition, and speech recognition.\n",
       "3. **Natural Language Processing (NLP):** NLP has improved significantly with the introduction of transformer-based models, enabling computers to understand and generate human-like language.\n",
       "4. **Robotics:** Robotics has seen significant advancements with the development of humanoid robots, autonomous vehicles, and robotic assistants.\n",
       "\n",
       "**Applications of AI:**\n",
       "\n",
       "1. **Virtual Assistants:** Virtual assistants like Siri, Alexa, and Google Assistant use AI to perform tasks such as voice recognition, scheduling appointments, and answering questions.\n",
       "2. **Healthcare:** AI is being used in healthcare for medical image analysis, disease diagnosis, and personalized medicine.\n",
       "3. **Transportation:** Self-driving cars and trucks are being developed using AI algorithms to improve safety and efficiency.\n",
       "4. **Cybersecurity:** AI-powered systems can detect and respond to cyber threats more effectively than traditional security measures.\n",
       "\n",
       "**Challenges and Concerns:**\n",
       "\n",
       "1. **Job Displacement:** The increasing use of automation and AI has raised concerns about job displacement and the need for re-skilling.\n",
       "2. **Bias and Fairness:** AI systems can perpetuate biases present in the data used to train them, highlighting the need for diversity and inclusion in AI development.\n",
       "3. **Security Risks:** As AI becomes more pervasive, it also poses new security risks, including the potential for AI-powered attacks on critical infrastructure.\n",
       "\n",
       "**Conclusion:**\n",
       "Artificial intelligence has made tremendous progress in recent years, transforming industries and revolutionizing the way we live and work. However, as AI continues to evolve, it's essential to address the challenges and concerns associated with its development and deployment.\n",
       "\n",
       "**Recommendations:**\n",
       "\n",
       "1. **Invest in Education:** Encourage education and re-skilling programs to prepare workers for an AI-driven future.\n",
       "2. **Promote Diversity and Inclusion:** Ensure that AI development teams are diverse and inclusive to mitigate biases and promote fairness.\n",
       "3. **Develop Secure AI Systems:** Prioritize the development of secure AI systems to protect against cyber threats.\n",
       "\n",
       "By addressing these challenges and concerns, we can unlock the full potential of AI to create a better future for all."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = chain.invoke(\"AI\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2688c6b2-1333-4a39-bb92-2c1ab1cc9924",
   "metadata": {},
   "source": [
    "# LCEL RunnableParallel and RunnablePassthrough\n",
    "LCEL provides us with various Runnable classes that allow us to control the flow of data and execution order through our chains. Two of these are RunnableParallel and RunnablePassthrough.\n",
    "\n",
    "- RunnableParallel — allows us to run multiple Runnable instances in parallel. Acting almost as a Y-fork in the chain.\n",
    "\n",
    "- RunnablePassthrough — allows us to pass through a variable to the next Runnable without modification.\n",
    "\n",
    "To see these runnables in action, we will create two data sources, each source provides specific information but to answer the question we will need both to fed to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7de860b8-5b27-41e9-89ee-4c2e0a5e22df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theha\\anaconda3\\envs\\langchain\\Lib\\site-packages\\pydantic\\_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "embedding = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "\n",
    "vecstore_a = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"half the info is here\",\n",
    "        \"DeepSeek-V3 was released in December 2024\"\n",
    "    ],\n",
    "    embedding=embedding\n",
    ")\n",
    "vecstore_b = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"the other half of the info is here\",\n",
    "        \"the DeepSeek-V3 LLM is a mixture of experts model with 671B parameters\"\n",
    "    ],\n",
    "    embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e4b1de6-0c5d-43f3-a2e7-9ca21ed0a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"\"\"Using the context provided, answer the user's question.\n",
    "Context:\n",
    "{context_a}\n",
    "{context_b}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38699c2a-8d53-4185-9985-91b7c32f6406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(prompt_str),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac37e8b9-13ea-4298-8b44-c276a828e32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "retriever_a = vecstore_a.as_retriever()\n",
    "retriever_b = vecstore_b.as_retriever()\n",
    "\n",
    "retrieval = RunnableParallel(\n",
    "    {\n",
    "        \"context_a\": retriever_a, \"context_b\": retriever_b, \"question\": RunnablePassthrough()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee86099-5e7d-4098-af1e-60f5bb8e755c",
   "metadata": {},
   "source": [
    "![LLM](images/llm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c71ff16-63d5-45dc-9a7c-a3997ef52eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = retrieval | prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9e9eb69-a5f8-4ff8-829b-9806e99b2c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The DeepSeek-V3 model uses a Mixture of Experts (MoE) architecture. Specifically, it has 671 billion parameters.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke(\n",
    "    \"what architecture does the model DeepSeek released in december use?\"\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe08fb39-0194-4ada-b4fc-b721b4066dde",
   "metadata": {},
   "source": [
    "With that we've seen how we can use RunnableParallel and RunnablePassthrough to control the flow of data and execution order through our chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdc4761-5923-4eef-9924-766790542d46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
